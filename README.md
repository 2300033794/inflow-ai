# InfoFlow AI

Enterprise Internal Knowledge Assistant powered by FastAPI, FAISS, and Retrieval Augmented Generation (RAG).

## Overview

InfoFlow AI allows employees to query internal company documents (HR policies, IT guides, PDFs, DOCX, TXT, CSV) and receive accurate, cited answers generated by an LLM.

## Project Structure

```
frontend/
  index.html
  package.json
  vite.config.js
  src/
    main.jsx
    App.jsx
    api/
      api.js
    components/
      ChatWindow.jsx
      ChatInput.jsx
      Message.jsx
      SourceList.jsx
      Header.jsx
      StatusIndicator.jsx
    styles/
      app.css

backend/
  app/
    main.py               # FastAPI application entry point
    core/
      config.py           # Environment configuration
      logging.py          # Logging setup
    api/
      routes/
        chat.py           # POST /chat
        ingest.py         # POST /ingest
        health.py         # GET /health
    services/
      chat_service.py     # Chat orchestration
      ingest_service.py   # Ingestion orchestration
    rag/
      loader.py           # Document loading (PDF, DOCX, TXT, CSV)
      chunker.py          # Text chunking
      embedder.py         # Sentence-transformer embeddings
      vector_store.py     # FAISS vector store
      retriever.py        # Similarity search retrieval
      generator.py        # RAG answer generation (OpenAI)
      ingestion.py        # Ingestion pipeline
    schemas/
      chat_schema.py      # Pydantic request/response models
    utils/
      helpers.py          # Utility functions
  data/                   # Place your documents here
  requirements.txt
  Dockerfile
```

## Quick Start

### 1. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

The backend reads environment variables from `backend/.env` (loaded by `backend/app/core/config.py`).

Create `backend/.env`:

```env
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3.2
VECTOR_DB_PATH=data/vector_store
DATA_PATH=data
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K=5
EMBEDDING_MODEL=all-MiniLM-L6-v2
CORS_ORIGINS=*
```

| Variable | Required | Purpose | Notes |
|---|---|---|---|
| `OLLAMA_BASE_URL` | Yes | Base URL for local Ollama server | App fails fast if Ollama is not reachable |
| `MODEL_NAME` | No | Ollama chat model to use | Default: `llama3.2` |
| `VECTOR_DB_PATH` | No | FAISS index location | Relative to `backend/` unless absolute path |
| `DATA_PATH` | No | Document ingestion folder | Default: `data` |
| `CHUNK_SIZE` | No | Chunk length | Integer |
| `CHUNK_OVERLAP` | No | Chunk overlap | Integer |
| `TOP_K` | No | Retrieval result count | Integer |
| `EMBEDDING_MODEL` | No | Embedding model name | Default: `all-MiniLM-L6-v2` |
| `CORS_ORIGINS` | No | Allowed frontend origins | Comma-separated list, or `*` |

### 3. Start Ollama

Install Ollama first: https://ollama.com/download

In one terminal:

```bash
ollama serve
```

In another terminal (one-time model download):

```bash
ollama pull llama3.2
```

> Keep Ollama running. The backend validates Ollama availability during startup.

### 4. Add Documents

Place your PDF, DOCX, TXT, or CSV files in `backend/data/`.

### 5. Run the Server

```bash
cd backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 6. Ingest Documents

```bash
curl -X POST http://localhost:8000/ingest
```

### 7. Test the Chat Endpoint

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the leave policy?"}'
```

Response:
```json
{
  "answer": "Employees are entitled to 20 days of annual leave...",
  "sources": ["hr_policy.pdf"]
}
```

## API Endpoints

| Method | Path      | Description                        |
|--------|-----------|------------------------------------|
| GET    | /health   | Health check                       |
| POST   | /ingest   | Ingest documents from `data/`      |
| POST   | /chat     | Query the knowledge base           |

## Docker

```bash
cd backend
docker build -t infoflow-ai .
docker run -p 8000:8000 --env OLLAMA_BASE_URL=http://<ollama-host>:11434 infoflow-ai
```

Use `host.docker.internal` as `<ollama-host>` on Docker Desktop (Mac/Windows).  
For Linux hosts, use your host machine IP or `--network=host`.

## Frontend

The React frontend provides a chat interface for querying the knowledge base.

### Setup

```bash
cd frontend
npm install
npm run dev
```

The dev server starts at `http://localhost:3000` and proxies API requests to the backend at `http://localhost:8000`.

### Build for Production

```bash
cd frontend
npm run build
npm run preview
```

## Tech Stack

- **Frontend**: React 18 + Vite + Axios
- **Backend**: FastAPI + Uvicorn
- **Vector DB**: FAISS (local)
- **Embeddings**: sentence-transformers (`all-MiniLM-L6-v2`)
- **LLM**: Ollama (`llama3.2` by default)
- **File Support**: PDF, DOCX, TXT, CSV
